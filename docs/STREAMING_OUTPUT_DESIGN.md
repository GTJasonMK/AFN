# çœŸæ­£çš„æµå¼è¾“å‡ºå®ç°æ–¹æ¡ˆ

> è®¾è®¡æ—¶é—´ï¼š2025-11-22
> é—®é¢˜ï¼šå½“å‰æ˜¯ä¼ªæµå¼ï¼ˆç­‰LLMå®Œæ•´å“åº”åæ‰æ‰“å­—ï¼‰ï¼Œéœ€è¦æ”¹ä¸ºçœŸæµå¼ï¼ˆLLMè¾“å‡ºä¸€ä¸ªtokenå°±ç«‹å³æ˜¾ç¤ºï¼‰

---

## é—®é¢˜åˆ†æ

### å½“å‰å®ç°ï¼ˆä¼ªæµå¼ï¼‰

```python
# åç«¯
llm_response = await llm_service.get_llm_response(...)  # ç­‰å¾…å®Œæ•´å“åº”
return {"ai_message": llm_response, ...}

# å‰ç«¯
def onSuccess(self, response):
    ai_message = response['ai_message']  # æ‹¿åˆ°å®Œæ•´æ–‡æœ¬
    self.addMessage(ai_message, typing_effect=True)  # æ‰“å­—æœºæ•ˆæœæ¨¡æ‹Ÿ
```

**é—®é¢˜**ï¼š
- âŒ ç”¨æˆ·éœ€è¦ç­‰å¾…LLMå®Œæ•´ç”Ÿæˆåæ‰èƒ½çœ‹åˆ°ç¬¬ä¸€ä¸ªå­—
- âŒ æ‰“å­—æœºæ•ˆæœåªæ˜¯è§†è§‰æ¨¡æ‹Ÿï¼Œä¸æ˜¯çœŸæ­£çš„æµå¼
- âŒ é•¿å›å¤æ—¶ç­‰å¾…æ—¶é—´è¿‡é•¿ï¼Œä½“éªŒå·®

### çœŸæµå¼å®ç°ç›®æ ‡

```python
# åç«¯
async for chunk in llm.stream():  # ç«‹å³yieldæ¯ä¸ªtoken
    yield f"data: {chunk}\n\n"  # SSEæ ¼å¼

# å‰ç«¯
eventSource.onmessage = (event) => {
    append_to_bubble(event.data)  # ç«‹å³æ˜¾ç¤ºæ¯ä¸ªtoken
}
```

**ä¼˜åŠ¿**ï¼š
- âœ… ç”¨æˆ·ç«‹å³çœ‹åˆ°ç¬¬ä¸€ä¸ªtokenå¼€å§‹è¾“å‡º
- âœ… æ„ŸçŸ¥å“åº”é€Ÿåº¦æ›´å¿«
- âœ… é•¿å›å¤æ—¶ä½“éªŒæµç•…

---

## æŠ€æœ¯æ–¹æ¡ˆï¼šSSE (Server-Sent Events)

### ä¸ºä»€ä¹ˆé€‰æ‹©SSEè€ŒéWebSocketï¼Ÿ

| ç‰¹æ€§ | SSE | WebSocket |
|------|-----|-----------|
| é€šä¿¡æ–¹å‘ | å•å‘ï¼ˆæœåŠ¡å™¨â†’å®¢æˆ·ç«¯ï¼‰ | åŒå‘ |
| åè®® | HTTP | ç‹¬ç«‹åè®® |
| æµè§ˆå™¨æ”¯æŒ | åŸç”Ÿæ”¯æŒEventSource | éœ€è¦WebSocket API |
| å¤æ‚åº¦ | ç®€å• | å¤æ‚ |
| é€‚ç”¨åœºæ™¯ | **LLMæµå¼è¾“å‡º** | èŠå¤©ã€æ¸¸æˆç­‰åŒå‘äº¤äº’ |

**ç»“è®º**ï¼šå¯¹äºLLMæµå¼è¾“å‡ºï¼ŒSSEæ˜¯æœ€ä½³é€‰æ‹©ã€‚

---

## æ¶æ„è®¾è®¡

### 1. åç«¯æµå¼æ¶æ„

```python
# backend/app/services/llm_service.py
class LLMService:
    async def get_llm_response_stream(
        self,
        system_prompt: str,
        conversation_history: List[Dict[str, str]],
        *,
        temperature: float = 0.7,
        user_id: Optional[int] = None,
        timeout: float = 300.0,
    ) -> AsyncGenerator[str, None]:
        """
        æµå¼ç”ŸæˆLLMå“åº”ï¼ˆå¼‚æ­¥ç”Ÿæˆå™¨ï¼‰

        Yields:
            str: æ¯ä¸ªtokençš„æ–‡æœ¬å†…å®¹
        """
        # è·å–é…ç½®
        config = await self._resolve_llm_config(user_id)

        # åˆ›å»ºå®¢æˆ·ç«¯
        client = LLMClient.create_from_config(config)
        messages = [{"role": "system", "content": system_prompt}, *conversation_history]
        chat_messages = ChatMessage.from_list(messages)

        # æµå¼è°ƒç”¨
        async for chunk in client.stream_chat(
            messages=chat_messages,
            model=config["model"],
            temperature=temperature,
            timeout=timeout,
        ):
            if chunk.get("content"):
                yield chunk["content"]  # ç›´æ¥yieldæ¯ä¸ªtoken
```

### 2. è·¯ç”±å±‚SSEç«¯ç‚¹

```python
# backend/app/api/routers/novels/inspiration.py
from fastapi.responses import StreamingResponse

@router.post("/{project_id}/inspiration/converse-stream")
async def converse_with_inspiration_stream(
    project_id: str,
    request: ConverseRequest,
    novel_service: NovelService = Depends(get_novel_service),
    llm_service: LLMService = Depends(get_llm_service),
    session: AsyncSession = Depends(get_session),
    desktop_user: UserInDB = Depends(get_default_user),
):
    """
    æµå¼çµæ„Ÿå¯¹è¯ç«¯ç‚¹ï¼ˆSSEï¼‰

    Returns:
        StreamingResponse with text/event-stream
    """
    async def event_generator():
        # 1. å‡†å¤‡å¯¹è¯ä¸Šä¸‹æ–‡
        conversation_service = ConversationService(session)
        history_records = await conversation_service.list_conversations(project_id)
        conversation_history = [
            {"role": record.role, "content": record.content}
            for record in history_records
        ]
        user_content = json.dumps(request.user_input, ensure_ascii=False)
        conversation_history.append({"role": "user", "content": user_content})

        # 2. å‡†å¤‡Prompt
        system_prompt = await prompt_service.get_prompt("inspiration")
        system_prompt = f"{system_prompt}\n{JSON_RESPONSE_INSTRUCTION}"

        # 3. æµå¼ç”ŸæˆAIå“åº”
        full_response = ""
        async for token in llm_service.get_llm_response_stream(
            system_prompt=system_prompt,
            conversation_history=conversation_history,
            temperature=settings.llm_temp_inspiration,
            user_id=desktop_user.id,
        ):
            full_response += token
            # å‘é€tokenäº‹ä»¶
            yield f"event: token\ndata: {json.dumps({'token': token})}\n\n"

        # 4. è§£æå®Œæ•´å“åº”å¹¶å‘é€metadata
        cleaned = remove_think_tags(full_response)
        normalized = unwrap_markdown_json(cleaned)
        parsed = parse_llm_json_or_fail(full_response, f"é¡¹ç›®{project_id}çš„çµæ„Ÿå¯¹è¯å“åº”è§£æå¤±è´¥")

        # 5. ä¿å­˜å¯¹è¯å†å²
        await conversation_service.append_conversation(project_id, "user", user_content)
        await conversation_service.append_conversation(project_id, "assistant", normalized)
        await session.commit()

        # 6. å‘é€å®Œæˆäº‹ä»¶ï¼ˆåŒ…å«ui_controlç­‰metadataï¼‰
        yield f"event: complete\ndata: {json.dumps(parsed, ensure_ascii=False)}\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # ç¦ç”¨Nginxç¼“å†²
        },
    )
```

### 3. å‰ç«¯SSEå®¢æˆ·ç«¯

```python
# frontend/windows/inspiration_mode/main.py
def onMessageSent(self, message):
    """ç”¨æˆ·å‘é€æ¶ˆæ¯ï¼ˆæµå¼ç‰ˆæœ¬ï¼‰"""
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    self.addMessage(message, is_user=True)

    # ç¦ç”¨è¾“å…¥
    self.input_widget.setEnabled(False)

    # åˆ›å»ºAIæ¶ˆæ¯æ°”æ³¡ï¼ˆåˆå§‹ä¸ºç©ºï¼‰
    self.current_ai_bubble = ChatBubble("", is_user=False)
    self.chat_layout.insertWidget(self.chat_layout.count() - 1, self.current_ai_bubble)

    # å¯åŠ¨SSEç›‘å¬
    self._start_sse_stream(message)

def _start_sse_stream(self, message):
    """å¯åŠ¨SSEæµå¼ç›‘å¬"""
    if not self.project_id:
        # åˆ›å»ºæ–°é¡¹ç›®
        response = self.api_client.create_novel(
            title="æœªå‘½åé¡¹ç›®",
            initial_prompt=message
        )
        self.project_id = response.get('id')

    # æ„é€ SSE URL
    url = f"{self.api_client.base_url}/api/novels/{self.project_id}/inspiration/converse-stream"

    # å¯åŠ¨SSEç›‘å¬çº¿ç¨‹
    worker = SSEWorker(url, {
        "user_input": {"message": message},
        "conversation_state": {}
    })
    worker.token_received.connect(self.on_token_received)
    worker.complete.connect(self.on_stream_complete)
    worker.error.connect(self.on_stream_error)
    worker.start()

def on_token_received(self, token):
    """æ”¶åˆ°ä¸€ä¸ªtoken"""
    # ç«‹å³è¿½åŠ åˆ°å½“å‰AIæ°”æ³¡
    current_text = self.current_ai_bubble.get_text()
    self.current_ai_bubble.set_text(current_text + token)

    # æ»šåŠ¨åˆ°åº•éƒ¨
    QTimer.singleShot(10, lambda: self.chat_scroll.verticalScrollBar().setValue(
        self.chat_scroll.verticalScrollBar().maximum()
    ))

def on_stream_complete(self, metadata):
    """æµå¼å“åº”å®Œæˆ"""
    # å¤„ç†ui_controlï¼ˆæ˜¾ç¤ºé€‰é¡¹å¡ç‰‡ï¼‰
    ui_control = metadata.get('ui_control', {})
    if ui_control.get('type') == 'inspired_options':
        options_data = ui_control.get('options', [])
        if options_data:
            self._add_inspired_options(options_data)
            placeholder = ui_control.get('placeholder', 'é€‰æ‹©ä¸Šé¢çš„é€‰é¡¹ï¼Œæˆ–è¾“å…¥ä½ çš„æ–°æƒ³æ³•...')
            self.input_widget.setPlaceholder(placeholder)

    # æ£€æŸ¥å¯¹è¯æ˜¯å¦å®Œæˆ
    self.is_conversation_complete = metadata.get('is_complete', False)

    # å¯ç”¨è¾“å…¥
    self.input_widget.setEnabled(True)
    self.input_widget.setFocus()
```

### 4. SSE Workerçº¿ç¨‹

```python
# frontend/utils/sse_worker.py
import json
import requests
from PyQt6.QtCore import QThread, pyqtSignal

class SSEWorker(QThread):
    """SSEæµå¼ç›‘å¬å·¥ä½œçº¿ç¨‹"""

    token_received = pyqtSignal(str)  # æ”¶åˆ°ä¸€ä¸ªtoken
    complete = pyqtSignal(dict)  # æµå¼å®Œæˆ
    error = pyqtSignal(str)  # é”™è¯¯

    def __init__(self, url, payload):
        super().__init__()
        self.url = url
        self.payload = payload
        self._stopped = False

    def run(self):
        """æ‰§è¡ŒSSEç›‘å¬"""
        try:
            # ä½¿ç”¨requestsçš„streamæ¨¡å¼
            with requests.post(
                self.url,
                json=self.payload,
                stream=True,
                headers={"Accept": "text/event-stream"},
                timeout=(10, None)  # è¿æ¥10ç§’è¶…æ—¶ï¼Œè¯»å–æ— é™åˆ¶
            ) as response:
                response.raise_for_status()

                # è§£æSSEæµ
                for line in response.iter_lines():
                    if self._stopped:
                        break

                    if not line:
                        continue

                    line = line.decode('utf-8')

                    # è§£æSSEäº‹ä»¶
                    if line.startswith('event: '):
                        event_type = line[7:]
                    elif line.startswith('data: '):
                        data = json.loads(line[6:])

                        if event_type == 'token':
                            # å‘å°„tokenä¿¡å·
                            self.token_received.emit(data['token'])
                        elif event_type == 'complete':
                            # å‘å°„å®Œæˆä¿¡å·
                            self.complete.emit(data)

        except Exception as e:
            self.error.emit(str(e))

    def stop(self):
        """åœæ­¢ç›‘å¬"""
        self._stopped = True
```

---

## å®ç°æ­¥éª¤

### Phase 1: åç«¯æµå¼æ”¯æŒ âœ…

1. **LLMServiceæ·»åŠ æµå¼ç”Ÿæˆå™¨**
   - æ–°å¢ `get_llm_response_stream()` æ–¹æ³•
   - è¿”å› `AsyncGenerator[str, None]`
   - ç›´æ¥yield LLMClient.stream_chat()çš„æ¯ä¸ªchunk

2. **Inspirationè·¯ç”±æ·»åŠ SSEç«¯ç‚¹**
   - æ–°å¢ `/converse-stream` ç«¯ç‚¹
   - ä½¿ç”¨ `StreamingResponse`
   - å‘é€ä¸¤ç§äº‹ä»¶ï¼š`token` å’Œ `complete`

### Phase 2: å‰ç«¯SSEå®¢æˆ·ç«¯ âœ…

3. **åˆ›å»ºSSEWorkerçº¿ç¨‹**
   - ç›‘å¬SSEæµ
   - è§£æäº‹ä»¶å¹¶å‘å°„PyQtä¿¡å·
   - æ”¯æŒåœæ­¢å’Œé”™è¯¯å¤„ç†

4. **ä¿®æ”¹InspirationModeå¯¹è¯é€»è¾‘**
   - æ›¿æ¢åŸæœ‰AsyncAPIWorkerä¸ºSSEWorker
   - å®ç° `on_token_received()` å®æ—¶è¿½åŠ æ–‡æœ¬
   - å®ç° `on_stream_complete()` å¤„ç†metadata

5. **ChatBubbleæ”¯æŒåŠ¨æ€æ›´æ–°**
   - æ–°å¢ `set_text()` æ–¹æ³•
   - æ–°å¢ `get_text()` æ–¹æ³•
   - ç¡®ä¿setTextä¸å½±å“æ ·å¼

### Phase 3: å…¼å®¹æ€§å¤„ç† âœ…

6. **ä¿ç•™åŸæœ‰éæµå¼ç«¯ç‚¹**
   - `/converse` - åŸæœ‰ç«¯ç‚¹ä¿ç•™ï¼ˆç”¨äºé™çº§ï¼‰
   - `/converse-stream` - æ–°çš„æµå¼ç«¯ç‚¹
   - å‰ç«¯ä¼˜å…ˆä½¿ç”¨æµå¼ï¼Œå¤±è´¥æ—¶fallbackåˆ°éæµå¼

7. **APIå®¢æˆ·ç«¯å‡çº§**
   - ArborisAPIClient æ·»åŠ  `inspiration_converse_stream()` æ–¹æ³•
   - è¿”å›SSE URLè€Œéç›´æ¥è°ƒç”¨

---

## æ”¶ç›Šä¸é£é™©

### é¢„æœŸæ”¶ç›Š âœ…

- âœ… **ç”¨æˆ·ä½“éªŒå¤§å¹…æå‡**ï¼šç«‹å³çœ‹åˆ°è¾“å‡ºï¼Œæ„ŸçŸ¥é€Ÿåº¦å¿«
- âœ… **é•¿å›å¤å‹å¥½**ï¼šé¿å…é•¿æ—¶é—´ç­‰å¾…
- âœ… **æŠ€æœ¯å…ˆè¿›æ€§**ï¼šç¬¦åˆç°ä»£AIåº”ç”¨æ ‡å‡†
- âœ… **æ‰©å±•æ€§å¼º**ï¼šä¸ºåç»­åŠŸèƒ½ï¼ˆå¦‚æ‰“æ–­ã€é‡æ–°ç”Ÿæˆï¼‰é“ºè·¯

### æ½œåœ¨é£é™© âš ï¸

- âš ï¸ **å¤æ‚åº¦å¢åŠ **ï¼šSSEè¿æ¥ç®¡ç†ã€é”™è¯¯å¤„ç†æ›´å¤æ‚
- âš ï¸ **JSONè§£ææŒ‘æˆ˜**ï¼šéœ€è¦ç­‰å¾…å®Œæ•´å“åº”æ‰èƒ½è§£æJSONï¼ˆai_messageå¯èƒ½åµŒå¥—åœ¨JSONä¸­ï¼‰
- âš ï¸ **å…¼å®¹æ€§æµ‹è¯•**ï¼šéœ€è¦æµ‹è¯•ç½‘ç»œä¸ç¨³å®šåœºæ™¯
- âš ï¸ **èµ„æºæ¶ˆè€—**ï¼šé•¿æ—¶é—´SSEè¿æ¥å ç”¨èµ„æº

### ç‰¹æ®ŠæŒ‘æˆ˜ï¼šJSONæ ¼å¼å“åº”

å½“å‰çµæ„Ÿå¯¹è¯è¿”å›JSONï¼š
```json
{
  "ai_message": "è¿™æ˜¯AIå›å¤çš„æ–‡æœ¬...",
  "ui_control": {...},
  "conversation_state": {},
  "is_complete": false
}
```

**é—®é¢˜**ï¼šLLMæµå¼è¾“å‡ºçš„æ˜¯JSONå­—ç¬¦ä¸²ï¼Œéœ€è¦ç­‰å¾…å®Œæ•´JSONæ‰èƒ½è§£æã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. **åˆ†ç¦»ai_message**ï¼šè®©LLMå…ˆè¾“å‡ºai_messageï¼Œå†è¾“å‡ºå…¶ä»–å­—æ®µ
2. **æ”¹é€ Prompt**ï¼šè¦æ±‚LLMæŒ‰ç‰¹å®šæ ¼å¼è¾“å‡ºï¼ˆä¸ç°å®ï¼Œéš¾ä»¥ä¿è¯ï¼‰
3. **åç«¯è§£æ+é‡ç»„** â­ æ¨èï¼š
   - åç«¯æ”¶é›†å®Œæ•´JSON
   - æå–ai_messageé€å­—ç¬¦yieldï¼ˆtokenäº‹ä»¶ï¼‰
   - æœ€åå‘é€å®Œæ•´metadataï¼ˆcompleteäº‹ä»¶ï¼‰

**å®ç°**ï¼š
```python
async def event_generator():
    # æ”¶é›†å®Œæ•´å“åº”
    full_response = ""
    async for token in llm_service.get_llm_response_stream(...):
        full_response += token

    # è§£æJSON
    parsed = json.loads(full_response)
    ai_message = parsed["ai_message"]

    # é€å­—ç¬¦å‘é€ai_message
    for char in ai_message:
        yield f"event: token\ndata: {json.dumps({'token': char})}\n\n"
        await asyncio.sleep(0.01)  # æ§åˆ¶å‘é€é€Ÿåº¦

    # å‘é€å®Œæ•´metadata
    yield f"event: complete\ndata: {json.dumps(parsed)}\n\n"
```

**é—®é¢˜**ï¼šè¿™æ ·ä»ç„¶éœ€è¦ç­‰å¾…LLMå®Œæ•´è¾“å‡ºåæ‰èƒ½å¼€å§‹æ˜¾ç¤ºğŸ˜

**çœŸæ­£çš„è§£å†³æ–¹æ¡ˆ** â­â­â­ï¼š
ä¿®æ”¹Promptï¼Œè®©LLMå…ˆè¾“å‡ºai_messageï¼ˆçº¯æ–‡æœ¬ï¼‰ï¼Œå†è¾“å‡ºJSONï¼š
```
è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›å¤ï¼š
1. å…ˆè¾“å‡ºå¯¹ç”¨æˆ·çš„å›å¤ï¼ˆçº¯æ–‡æœ¬ï¼‰
2. ç„¶åè¾“å‡º <JSON>...</JSON> æ ‡è®°åŒ…è£¹çš„JSONæ•°æ®

ç¤ºä¾‹ï¼š
ä½ å¥½ï¼æˆ‘ç†è§£ä½ çš„åˆ›æ„äº†ã€‚è®©æˆ‘ä¸ºä½ æä¾›å‡ ä¸ªæ–¹å‘...

<JSON>
{
  "ui_control": {...},
  "conversation_state": {},
  "is_complete": false
}
</JSON>
```

è¿™æ ·åç«¯å¯ä»¥ï¼š
1. æµå¼yieldçº¯æ–‡æœ¬éƒ¨åˆ†ï¼ˆå®æ—¶æ˜¾ç¤ºï¼‰
2. æ”¶é›†åˆ°<JSON>æ ‡è®°åè§£æmetadata3. å‘é€completeäº‹ä»¶

---

## æ¨èçš„å®ç°ç­–ç•¥

ç”±äºJSONæ ¼å¼çš„å¤æ‚æ€§ï¼Œæˆ‘å»ºè®®ï¼š

**é˜¶æ®µ1ï¼šä¿å®ˆå®ç°**ï¼ˆæ¨èç«‹å³æ‰§è¡Œï¼‰
- ä¿æŒå½“å‰JSONå“åº”æ ¼å¼
- åç«¯æ”¶é›†å®Œæ•´å“åº”åé€å­—ç¬¦å‘é€ï¼ˆæ¨¡æ‹Ÿæµå¼ï¼‰
- è™½ç„¶ä»éœ€ç­‰å¾…LLMå®Œæ•´ç”Ÿæˆï¼Œä½†ï¼š
  - ä»£ç æ¶æ„ä¸ºçœŸæµå¼åšå¥½å‡†å¤‡
  - å‰ç«¯ä½“éªŒå·²æ˜¯çœŸæµå¼ï¼ˆé€å­—ç¬¦æ˜¾ç¤ºï¼‰
  - åç»­åªéœ€ä¼˜åŒ–åç«¯å³å¯

**é˜¶æ®µ2ï¼šPromptæ”¹é€ **ï¼ˆåç»­ä¼˜åŒ–ï¼‰
- ä¿®æ”¹Promptæ¨¡æ¿ï¼Œè¦æ±‚å…ˆè¾“å‡ºçº¯æ–‡æœ¬å†è¾“å‡ºJSON
- åç«¯å®ç°çœŸæ­£çš„tokençº§æµå¼
- è¿™éœ€è¦å¤§é‡æµ‹è¯•ç¡®ä¿LLMéµå®ˆæ ¼å¼

---

## æ–‡ä»¶ä¿®æ”¹æ¸…å•

### åç«¯ä¿®æ”¹ï¼ˆ5ä¸ªæ–‡ä»¶ï¼‰

1. `backend/app/services/llm_service.py`
   - æ–°å¢ `get_llm_response_stream()` æ–¹æ³•

2. `backend/app/api/routers/novels/inspiration.py`
   - æ–°å¢ `/converse-stream` ç«¯ç‚¹
   - å¯¼å…¥ `StreamingResponse`

3. `backend/app/utils/sse_helpers.py` â­ æ–°å»º
   - SSEäº‹ä»¶æ ¼å¼åŒ–å·¥å…·
   - `sse_event(event_type, data)` å‡½æ•°

### å‰ç«¯ä¿®æ”¹ï¼ˆ5ä¸ªæ–‡ä»¶ï¼‰

4. `frontend/utils/sse_worker.py` â­ æ–°å»º
   - SSEWorkerçº¿ç¨‹ç±»
   - ç›‘å¬SSEæµå¹¶å‘å°„ä¿¡å·

5. `frontend/windows/inspiration_mode/main.py`
   - ä¿®æ”¹ `onMessageSent()` ä½¿ç”¨SSEWorker
   - æ–°å¢ `on_token_received()` å¤„ç†token
   - æ–°å¢ `on_stream_complete()` å¤„ç†complete
   - æ–°å¢ `_start_sse_stream()` å¯åŠ¨SSE

6. `frontend/windows/inspiration_mode/chat_bubble.py`
   - æ–°å¢ `set_text()` æ–¹æ³•
   - æ–°å¢ `get_text()` æ–¹æ³•

7. `frontend/api/client.py`
   - æ–°å¢ `inspiration_converse_stream()` æ–¹æ³•ï¼ˆå¯é€‰ï¼‰

### æ–‡æ¡£ï¼ˆ1ä¸ªæ–‡ä»¶ï¼‰

8. `docs/STREAMING_OUTPUT_IMPLEMENTATION.md` â­ æœ¬æ–‡æ¡£

---

## æµ‹è¯•è®¡åˆ’

### å•å…ƒæµ‹è¯•

- [ ] LLMService.get_llm_response_stream() è¿”å›AsyncGenerator
- [ ] SSEäº‹ä»¶æ ¼å¼æ­£ç¡®ï¼ˆevent: token/completeï¼‰
- [ ] JSONè§£æå’Œmetadataæå–æ­£ç¡®

### é›†æˆæµ‹è¯•

- [ ] å®Œæ•´å¯¹è¯æµç¨‹ï¼šç”¨æˆ·è¾“å…¥ â†’ SSEæµ â†’ å‰ç«¯æ˜¾ç¤º
- [ ] tokené€ä¸ªæ­£ç¡®æ˜¾ç¤º
- [ ] completeäº‹ä»¶åŒ…å«å®Œæ•´ui_control
- [ ] é€‰é¡¹å¡ç‰‡æ­£ç¡®æ˜¾ç¤º
- [ ] å¯¹è¯å†å²æ­£ç¡®ä¿å­˜

### å¼‚å¸¸åœºæ™¯æµ‹è¯•

- [ ] ç½‘ç»œä¸­æ–­æ—¶SSEé‡è¿
- [ ] LLMè¶…æ—¶å¤„ç†
- [ ] JSONè§£æå¤±è´¥é™çº§
- [ ] å‰ç«¯åˆ‡æ¢é¡µé¢æ—¶åœæ­¢SSE

---

## å…¼å®¹æ€§è¯´æ˜

**å‘åå…¼å®¹**ï¼š
- âœ… ä¿ç•™åŸæœ‰ `/converse` éæµå¼ç«¯ç‚¹
- âœ… å‰ç«¯ä¼˜å…ˆä½¿ç”¨æµå¼ï¼Œå¤±è´¥æ—¶fallback
- âœ… ç§»åŠ¨ç«¯æˆ–æ—§æµè§ˆå™¨ä»å¯ä½¿ç”¨éæµå¼

**æµè§ˆå™¨æ”¯æŒ**ï¼š
- âœ… Chrome/Edge (Chromium): å®Œå…¨æ”¯æŒ
- âœ… Firefox: å®Œå…¨æ”¯æŒ
- âœ… Safari: å®Œå…¨æ”¯æŒï¼ˆiOS 13+ï¼‰
- âš ï¸ IE11: ä¸æ”¯æŒï¼ˆä½†æ¡Œé¢åº”ç”¨ä¸æ¶‰åŠï¼‰

---

**è®¾è®¡è€…**ï¼šClaude Code
**è®¾è®¡æ—¶é—´**ï¼š2025-11-22
**å®æ–½ä¼˜å…ˆçº§**ï¼šé«˜
**é¢„ä¼°å·¥ä½œé‡**ï¼š4-6å°æ—¶ï¼ˆé˜¶æ®µ1ï¼‰ï¼Œ2-3å°æ—¶ï¼ˆé˜¶æ®µ2ï¼‰
**çŠ¶æ€**ï¼šğŸ“‹ è®¾è®¡å®Œæˆï¼Œå¾…ç¡®è®¤å®æ–½
